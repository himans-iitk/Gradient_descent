{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAME: Himanshu Mishra                                         Roll No: 180293\n",
    "# Assignment # 2                                                          CS771A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient,init_,learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate* gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gradient descent for finding minima of functions\n",
    "\n",
    "(i) $x^2 + 3x + 4$\n",
    "\n",
    "(ii) $x^4 - 3x^2 + 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima for function x^2 + 3x + 4 is: -1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Minima for function x^2 + 3x + 4 is:\", gradient_descent(gradient=lambda v: 2 * v + 3, init_=4.0, learn_rate=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima for function x^2 + 3x + 4 is: -1.094\n"
     ]
    }
   ],
   "source": [
    "print(\"Minima for function x^2 + 3x + 4 is:\",gradient_descent(gradient=lambda v: 4*(v**3) - 6*v + 2, init_= -0.5, learn_rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient function to calculate gradients for a linear regression y = ax + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_(X,y,a,b):\n",
    "    N = len(X)\n",
    "    f = y - (a*X + b)\n",
    "    gr1 = (-2 * X.dot(f).sum() / N)\n",
    "    gr2 = (-2 * f.sum() / N)\n",
    "    \n",
    "    return gr1,gr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificial data being generated\n",
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5\n",
    "res = 1.5 * np.random.randn(10000)\n",
    "y = 2 + 0.3 * X + res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is implemented to obtain the optimal values for (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal values of parameters a and b are:\n",
      "a =  0.30418945986869633\n",
      "b =  1.977579938205467\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0\n",
    "for _ in range(50):\n",
    "    gr1,gr2 = gradient_(X,y,a,b)\n",
    "    delta1 = -0.05*gr1\n",
    "    delta2 = -0.05*gr2\n",
    "    if np.all((np.abs(delta1) <= 1e-06) or (np.abs(delta2) <= 1e-06) ):\n",
    "            break\n",
    "    a += delta1\n",
    "    b += delta2\n",
    "\n",
    "print(\"The optimal values of parameters a and b are:\")\n",
    "print(\"a = \", a)\n",
    "print(\"b = \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch Stochastic Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, y, lr=0.05, epoch=100, batch_size=50):\n",
    "        \n",
    "    '''\n",
    "    Stochastic Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    a, b = 0, 0 # initial parameters\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(epoch):\n",
    "        \n",
    "        indexes = np.random.randint(0, len(X), batch_size) # random sample\n",
    "        \n",
    "        Xs = np.take(X, indexes)\n",
    "        ys = np.take(y, indexes)\n",
    "        N = len(Xs)\n",
    "        \n",
    "        f = ys - (a*Xs + b)\n",
    "        \n",
    "        # Updating parameters m and b\n",
    "        delta1 = -lr * (-2 * Xs.dot(f).sum() / N)\n",
    "        delta2 = -lr * (-2 * f.sum() / N)\n",
    "        if np.all((np.abs(delta1) <= 1e-06) or (np.abs(delta2) <= 1e-06) ):\n",
    "            break\n",
    "        a += delta1\n",
    "        b += delta2\n",
    "    \n",
    "    end = time.time()\n",
    "    return a, b , (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of a, b and time(t) for minibatch stochastic gradient descent are:\n",
      "a: 0.2446245170756185\n",
      "b: 2.0633179179293633\n",
      "t: 0.00797891616821289\n"
     ]
    }
   ],
   "source": [
    "a,b,t = SGD(X,y)\n",
    "print(\"The values of a, b and time(t) for minibatch stochastic gradient descent are:\")\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"t:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of a, b and time(t) for stochastic gradient descent are:\n",
      "a: 0.07531456210237986\n",
      "b: 2.5157944584139\n",
      "t: 0.007977485656738281\n"
     ]
    }
   ],
   "source": [
    "a,b,t = SGD(X,y, batch_size = 1)\n",
    "print(\"The values of a, b and time(t) for stochastic gradient descent are:\")\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"t:\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bullet$ The Mini-batch Schohastic Gradient descent works slightly faster/comparable in terms of time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 0, 0, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = np.random.randint(0,5, size=len(X))## check if this gives approxequal data across all splits\n",
    "kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.91013086, 3.46986372, 4.        ],\n",
       "       [2.50039302, 1.50027141, 1.        ],\n",
       "       [3.94684496, 5.78445386, 2.        ],\n",
       "       ...,\n",
       "       [2.79218045, 2.75176526, 0.        ],\n",
       "       [1.41769827, 0.83691259, 0.        ],\n",
       "       [4.74527858, 2.93379091, 3.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = np.vstack((X,y,kfold)).T\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sz = 100\n",
    "error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b, X, y):\n",
    "    y_pred = np.zeros(len(X))\n",
    "    error = 0\n",
    "    N = len(X)\n",
    "    for i in range(len(X)):\n",
    "        y_pred[i] = a*X[i] + b\n",
    "        error += (y[i]-y_pred[i])*(y[i]-y_pred[i])\n",
    "    \n",
    "    return error/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    train_idx = combined[combined[:,2]!=fold]\n",
    "    val_idx = combined[combined[:,2]==fold]\n",
    "    X_train = train_idx[:,0]\n",
    "    y_train = train_idx[:,1]\n",
    "    X_val = val_idx[:,0]\n",
    "    y_val = val_idx[:,1]\n",
    "    temp = []\n",
    "    for sz in range(1,max_sz+1):\n",
    "        a,b,t = SGD(X_train,y_train, batch_size = sz)\n",
    "        temp.append(MSE(a,b,X_val,y_val))\n",
    "    \n",
    "    error.append(temp)\n",
    "\n",
    "er = np.array(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal batch size for Minibatch SDG is: 92\n"
     ]
    }
   ],
   "source": [
    "best_sz = 0\n",
    "mini = 1e7\n",
    "for i in range(max_sz):\n",
    "    avg_error = (er[0][i] + er[1][i] + er[2][i]+ er[3][i] + er[4][i])/5\n",
    "    if avg_error<mini:\n",
    "        mini = avg_error\n",
    "        best_sz = i\n",
    "print(\"The optimal batch size for Minibatch SDG is:\",best_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Finding and Interpretations:}$\n",
    "\n",
    "The Mini_batch SGD works slightly faster than the SDG for the data used and Mini-Batch GD is much stable than the SGD, therefore this algorithm gives parameter values much closer to the minimum than SGD. The optimal batch size for the minibatch SDG is also calculted using k-fold croos-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also, the optimal batch size for minibatch SGD turns out to be 92\n"
     ]
    }
   ],
   "source": [
    "print(\"Also, the optimal batch size for minibatch SGD turns out to be\", best_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\textbf{Part (i)}$: \n",
    " \n",
    " Let P(C) be the probability of someone having cold and let P(F) be the probability of someone having having fever.\n",
    " \n",
    " From the provided baysian network we have: \n",
    " \n",
    " The probability of having cold P(C) = 0.02\n",
    " \n",
    " The probability of having fever having fever when they have cold P(F|C) = 0.307\n",
    " \n",
    " So, the probability of having cold and fever will be :\n",
    " \n",
    "$$ P(C,F) = P(C)*P(F|C)$$\n",
    " \n",
    " $$P(C,F) = 0.02*0.307$$\n",
    " \n",
    " $$\\mathbf{P(C,F) = 0.00614}$$\n",
    " \n",
    " So, probability of someone having both cold and fever is $\\textbf{0.00614}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part(ii)}$\n",
    "\n",
    "Let P(C) be the probability of having cold, P(Co) be the probability of having Cough and P(L) be the probability of having lung disease.\n",
    "\n",
    "From the given bayesian netwok,\n",
    "\n",
    "The probability of having lung disease:\n",
    "\n",
    "$$P(L) = 0.2 * 0.1009 + 0.8 * 0.001$$\n",
    "\n",
    "$$\\mathbf{P(L) = 0.02098}$$\n",
    "\n",
    "also, $$P(L)' = 1 - P(L) = 1 - 0.02098 $$\n",
    "\n",
    "$$\\mathbf{P(L)' = 0.97902}$$\n",
    "\n",
    "Now, The probability of having both Cough and Cold will be:\n",
    "\n",
    "$$P(C, Co) = P(L)*P(C) * 0.7525 + P(L)'*P(C) * 0.0505$$\n",
    "\n",
    "$$P(C, Co) = 0.02098 * 0.02 * 0.7525 + 0.97902 * 0.02 * 0.0505$$\n",
    "\n",
    "$$P(C, Co) = 0.0101968$$\n",
    "\n",
    "Similarily, the probability of having cough P(Co) will be:\n",
    "\n",
    "$$P(Co) = P(L)*P(C) * 0.7525 + P(L)*P(C)' * 0.0505 + P(L)'*P(C) * 0.505 + P(L)'P(C)' * 0.01$$\n",
    "\n",
    "where $$P(C) = 0.02, P(C)' = 0.98, P(L) = 0.02098, P(L)' = 0.97902$$\n",
    "\n",
    "Thus, $$\\mathbf{P(Co) = 0.0301742}$$\n",
    "\n",
    "Now, $$\\mathbf{P(C|Co) = P(C,Co)/P(Co)}$$\n",
    "\n",
    "$$P(C|Co) = 0.0101968/0.0301742$$\n",
    "\n",
    "$$\\mathbf{P(C|Co) = 0.3379}$$\n",
    "\n",
    "Thus the probability of someone who has cough having cold is $$\\mathbf{0.3379}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let <strong> $X$ </strong> be a random variable of k-sided multinomial distribution. Then\n",
    "\n",
    "\n",
    "$$P(\\mathbf{X = x};n, \\mathbf{p}) = \\binom{n}{x_1,...,x_k}\\prod_{x = 1}^{k} p^{x_i}_i $$\n",
    "\n",
    "$$ = n!\\prod_{x = 1}^{k} \\frac{p^{x_i}_i}{x_i!}$$\n",
    "\n",
    "where $x_i$ is the number of success of the $k^{th}$ category in $n$ random draws, where $p_k$ is the probability of success of the $k^{th}$ category. Where,\n",
    "\n",
    "$$\\sum_{k = 1}^{K} x_k = n$$\n",
    "\n",
    "$$\\sum_{k = 1}^{K} p_k = 1$$\n",
    "\n",
    "Thus, the likelihood for multinomial fuction is given as\n",
    "\n",
    "$$L(\\mathbf{p}) = \\binom{n}{x_1,...,x_k}\\prod_{x = 1}^{k} p^{x_i}_i$$\n",
    "\n",
    "$$ = n!\\prod_{x = 1}^{k} \\frac{p^{x_i}_i}{x_i!}$$\n",
    "\n",
    "and the log-likelihood is\n",
    "\n",
    "$$ l(\\mathbf{p}) = logL(\\mathbf{p}) =  log\\left(n!\\prod_{x = 1}^{k} \\frac{p^{x_i}_i}{x_i!}\\right)$$\n",
    "\n",
    "$$logn! + log\\prod_{x = 1}^{k} \\frac{p^{x_i}_i}{x_i!}$$\n",
    "\n",
    "$$logn! + \\sum_{x = 1}^{k}log\\frac{p^{x_i}_i}{x_i!}$$\n",
    "\n",
    "$$logn! + \\sum_{x = 1}^{k}x_ilogp_i - \\sum_{x = 1}^{k}logx_i!$$\n",
    "\n",
    "Posing a constraint $\\left(\\sum_{i = 1}^{k} p_i = 1 \\right)$ with Lagrange multiplier\n",
    "$$l'(\\mathbf{p}, \\lambda) = l(\\mathbf{p}) + \\lambda\\left(1 - \\sum_{i = 1}^{k} p_i \\right) $$\n",
    "\n",
    "Now, for $\\mathbf{arg max_p} L(\\mathbf{p}, \\lambda)$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial }{\\partial p_i}l'(\\mathbf{p}, \\lambda) = \\displaystyle \\frac{\\partial }{\\partial p_i}l(\\mathbf{p}) + \\displaystyle \\frac{\\partial }{\\partial p_i} \\lambda \\left(1 - \\sum_{i = 1}^{k} p_i\\right) = 0$$\n",
    "$$\\displaystyle \\frac{\\partial }{\\partial p_i}\\sum_{i = 1}^{k}x_ilogp_i - \\lambda\\displaystyle \\frac{\\partial }{\\partial p_i}\\sum_{i = 1}^{k} p_i = 0$$\n",
    "\n",
    "$$ \\frac{x_i}{p_i} - \\lambda = 0 $$\n",
    "$$ p_i = \\frac{x_i}{\\lambda} $$\n",
    "$$ \\sum_{i = 1}^{k} p_i = \\sum_{i = 1}^{k} \\frac{x_i}{\\lambda}$$\n",
    "$$1 = \\frac{1}{\\lambda}\\sum_{i = 1}^{k}x_i$$\n",
    "$$ \\lambda = n$$\n",
    "Thus, \n",
    "$$p_i = \\frac{x_i}{n}$$\n",
    "S, finally the probability distribution that maximizes multinomial distribution likelihood function is\n",
    "\n",
    "$$\\mathbf{p} = \\left(\\frac{x_i}{n},...,\\frac{x_m}{n}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
